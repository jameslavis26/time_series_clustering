{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10d447d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-17 17:32:47.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtime_series.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mPROJ_ROOT path is: /home/james/Repo/PhD Repo/time_series_clustering\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time_series.time_series_models import KernelRidgeRegression\n",
    "from time_series.kernels import GaussianKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecbc92d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import silhouette_score, rand_score, adjusted_rand_score\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02a75542",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from scipy.spatial.distance import squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0103150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44a4f927",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/Repo/PhD Repo/time_series_clustering/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adcecc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHolder:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.update(**kwargs)\n",
    "    \n",
    "    def update(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a03c286",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptunaHyperparameterTuning:\n",
    "    def __init__(self, estimator, param_grid, scoring=None, nfolds=5, n_trials=30):\n",
    "        self.estimator = estimator\n",
    "        self.param_grid = param_grid\n",
    "        self.scoring = scoring if scoring else mean_squared_error\n",
    "        self.nfolds = nfolds\n",
    "        self.n_trials = n_trials\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        def objective(trial):\n",
    "            parameters = dict()\n",
    "\n",
    "            for p_name, p_params in self.param_grid.items():\n",
    "                if p_params[\"type\"] == \"float\":\n",
    "                    step = p_params[\"step\"] if \"step\" in p_params else None\n",
    "                    parameters[p_name] = trial.suggest_float(p_name, p_params[\"min\"], p_params[\"max\"], step=step)\n",
    "            \n",
    "            model = self.estimator(**parameters)\n",
    "\n",
    "            kf = KFold(n_splits=self.nfolds, random_state=None, shuffle=False)\n",
    "            scores = []\n",
    "            \n",
    "            for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "                X_train = X[train_index]\n",
    "                X_test = X[test_index]\n",
    "\n",
    "                y_train = y[train_index]\n",
    "                y_test = y[test_index] \n",
    "\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                score = self.scoring(y_test, y_pred)\n",
    "                scores.append(score)\n",
    "            \n",
    "            cv_score = np.mean(scores)\n",
    "            return cv_score\n",
    "\n",
    "\n",
    "        study = optuna.create_study()\n",
    "        study.optimize(\n",
    "            objective,\n",
    "            n_trials=self.n_trials,\n",
    "        )\n",
    "\n",
    "        self.best_params = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4e09c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridSearch:\n",
    "    def __init__(self, estimator, param_grid, scoring=None, nfolds=5):\n",
    "        self.estimator = estimator\n",
    "        self.param_grid = param_grid\n",
    "        self.scoring = scoring if scoring else mean_squared_error\n",
    "        self.nfolds = nfolds\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        param_names = list(self.param_grid.keys())\n",
    "        param_values = list(itertools.product(*self.param_grid.values()))\n",
    "\n",
    "        result = []\n",
    "\n",
    "        best_score = np.inf\n",
    "        best_params = []\n",
    "        \n",
    "        for p in param_values:\n",
    "            mapping = {param_names[i]:p[i] for i in range(len(param_names))}\n",
    "            model = self.estimator(**mapping)\n",
    "\n",
    "            kf = KFold(n_splits=self.nfolds, random_state=None, shuffle=False)\n",
    "            scores = []\n",
    "            \n",
    "            for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "                X_train = X[train_index]\n",
    "                X_test = X[test_index]\n",
    "\n",
    "                y_train = y[train_index]\n",
    "                y_test = y[test_index] \n",
    "\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                score = self.scoring(y_test, y_pred)\n",
    "                scores.append(score)\n",
    "            \n",
    "            cv_score = np.mean(scores)\n",
    "            result.append(\n",
    "                (mapping, cv_score)\n",
    "            )\n",
    "\n",
    "            if cv_score < best_score:\n",
    "                best_score = cv_score\n",
    "                best_params = p\n",
    "        \n",
    "        self.best_score = best_score\n",
    "        self.best_params = best_params\n",
    "\n",
    "        self.cv_results = result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5de920a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KRRWrapper(BaseEstimator):\n",
    "    def __init__(self, bandwidth, reg):\n",
    "        self.model = KernelRidgeRegression(\n",
    "            kernels=[GaussianKernel(bandwidth=bandwidth)],\n",
    "            reg = reg\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d85056fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series(x0:list, f, N=200, epsilon=0):\n",
    "    x = np.zeros(shape=(N))\n",
    "    for i in range(len(x0)):\n",
    "        x[i] = x0[i]\n",
    "\n",
    "    for i in range(len(x0), N):\n",
    "        x[i] = f(x[:i]) + np.random.normal(0, epsilon)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7184796",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamics = [\n",
    "    lambda x: np.sin(1*x[-1]),\n",
    "    lambda x: np.sin(2*x[-1]),\n",
    "    lambda x: np.sin(4*x[-1]),\n",
    "]\n",
    "\n",
    "paramsearch = OptunaHyperparameterTuning(\n",
    "    KRRWrapper,\n",
    "    param_grid={\n",
    "        \"bandwidth\": dict(type=\"float\", min=1e-9, max=4),\n",
    "        \"reg\": dict(type=\"float\", min=1e-12, max=1e-2)\n",
    "    },\n",
    "    scoring=mean_squared_error,\n",
    "    n_trials=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64f1c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1\n",
    "n_sets = 10\n",
    "lag = 1\n",
    "\n",
    "datasets = []\n",
    "\n",
    "for d_id, fd in enumerate(dynamics):\n",
    "    for i in range(n_sets):\n",
    "        x0 = np.random.random()*2\n",
    "        d = generate_time_series(x0, fd, 300, epsilon=epsilon)\n",
    "        datasets.append(\n",
    "            DataHolder(\n",
    "                data=d,\n",
    "                label=d_id\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7056e409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "for d in tqdm.tqdm(datasets):\n",
    "    # N = len(d.data) - 1\n",
    "    # x_train, y_train, x_test, y_test = d.data[:-1][:N//2], d.data[1:][:N//2], d.data[:-1][N//2:], d.data[1:][N//2:]\n",
    "    x_train, y_train = d.data[:-1], d.data[1:]\n",
    "\n",
    "    paramsearch.fit(x_train, y_train)\n",
    "    bandwidth = paramsearch.best_params[\"bandwidth\"]\n",
    "    reg = paramsearch.best_params[\"reg\"]\n",
    "\n",
    "    model = KernelRidgeRegression(\n",
    "        kernels=[GaussianKernel(bandwidth=bandwidth)],\n",
    "        reg = reg,\n",
    "        lag=lag\n",
    "    )\n",
    "\n",
    "    model.fit(x_train)\n",
    "\n",
    "    # print(mean_squared_error(model.predict(x_test), y_test[1:]))\n",
    "    \n",
    "    d.update(\n",
    "        bandwidth=bandwidth,\n",
    "        model=model,\n",
    "        kernel=model.kernels[0],\n",
    "        alpha=model.alpha\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647bcc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering 1\n",
    "similarity_matrix = np.zeros((len(datasets), len(datasets)))\n",
    "distance_matrix = np.zeros((len(datasets), len(datasets)))\n",
    "\n",
    "kernel = GaussianKernel(bandwidth=1)\n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    dii = (datasets[i].alpha.T@kernel(datasets[i].data[:-lag-1]/datasets[i].bandwidth, datasets[i].data[:-lag-1]/datasets[i].bandwidth)@datasets[i].alpha)[0][0]\n",
    "    for j in range(i, len(datasets)):\n",
    "        djj = (datasets[j].alpha.T@kernel(datasets[j].data[:-lag-1]/datasets[j].bandwidth, datasets[j].data[:-lag-1]/datasets[j].bandwidth)@datasets[j].alpha)[0][0]\n",
    "\n",
    "        dij = (datasets[i].alpha.T@kernel(datasets[i].data[:-lag-1]/datasets[i].bandwidth, datasets[j].data[:-lag-1]/datasets[j].bandwidth)@datasets[j].alpha)[0][0]\n",
    "\n",
    "        similarity_matrix[i][j] = dij\n",
    "        distance_matrix[i][j] = dii + djj - 2*dij\n",
    "        # similarity_matrix[i][j] = dij/np.sqrt(dii*djj) # Cosine distance\n",
    "        similarity_matrix[j][i] = similarity_matrix[i][j]\n",
    "        distance_matrix[j][i] = distance_matrix[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1381e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph laplacian\n",
    "D = np.diag(similarity_matrix.sum(axis=0))\n",
    "\n",
    "L = D - similarity_matrix\n",
    "\n",
    "evals, evecs = np.linalg.eig(L)\n",
    "eval_idx = np.argsort(evals)\n",
    "\n",
    "evals = evals[eval_idx]\n",
    "evecs = evecs[:, eval_idx]\n",
    "\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "elbow_idx = (np.where(evals[1:] - evals[:-1] > 1e-6)[0] + 1)[1] # Find the first instance where the increase in eval size is less than 1e-6. Call this the elbow\n",
    "labels = kmeans.fit_predict(np.abs(evecs[:, :elbow_idx]))\n",
    "\n",
    "cluster_labels = pd.Series(labels)\n",
    "\n",
    "for c in cluster_labels.unique():\n",
    "    cluster_indices = cluster_labels[cluster_labels == c].index\n",
    "    cluster_truth_labels = [datasets[i].label for i in cluster_indices]\n",
    "\n",
    "    print(f\"{c}: {cluster_truth_labels}\")\n",
    "\n",
    "print(adjusted_rand_score(\n",
    "    labels_true=[d.label for d in datasets],\n",
    "    labels_pred=labels\n",
    "), rand_score(\n",
    "    labels_true=[d.label for d in datasets],\n",
    "    labels_pred=labels  \n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be35e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=4, ncols=4, figsize=(15, 15))\n",
    "\n",
    "for n in range(len(dynamics)):\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            ax[i, j].scatter(evecs[n*n_sets:(n+1)*n_sets, i], evecs[n*n_sets:(n+1)*n_sets, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b33ccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f2ddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance_matrix[distance_matrix < 0] = 0\n",
    "\n",
    "# Assuming D is your full distance matrix\n",
    "condensed_D = squareform(distance_matrix)  # if D is a full matrix\n",
    "Z = linkage(condensed_D, method='average')\n",
    "labels = fcluster(Z, t=len(dynamics), criterion='maxclust')  # t is number of clusters\n",
    "\n",
    "cluster_labels = pd.Series(labels)\n",
    "\n",
    "for c in cluster_labels.unique():\n",
    "    cluster_indices = cluster_labels[cluster_labels == c].index\n",
    "    cluster_truth_labels = [datasets[i].label for i in cluster_indices]\n",
    "\n",
    "    print(f\"{c}: {cluster_truth_labels}\")\n",
    "\n",
    "adjusted_rand_score(\n",
    "    labels_true=[d.label for d in datasets],\n",
    "    labels_pred=labels\n",
    "), rand_score(\n",
    "    labels_true=[d.label for d in datasets],\n",
    "    labels_pred=labels  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4704ec6b",
   "metadata": {},
   "source": [
    "# Repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c93f19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamics = []\n",
    "\n",
    "dynamics.append(lambda x: np.sin(np.random.choice(np.linspace(0.1, 2, 20))*x[-1]))\n",
    "\n",
    "dynamics.append(\n",
    "    lambda x: np.exp(-(x[-1] - 1)**2),\n",
    ")\n",
    "\n",
    "dynamics.append(\n",
    "    lambda x: np.exp(-(x[-1])**2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8e3f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1\n",
    "n_sets = 20\n",
    "lag = 1\n",
    "\n",
    "datasets = []\n",
    "\n",
    "for d_id, fd in enumerate(dynamics):\n",
    "    for i in range(n_sets):\n",
    "        x0 = np.random.random()*2\n",
    "        d = generate_time_series(x0, fd, 300, epsilon=epsilon)\n",
    "        datasets.append(\n",
    "            DataHolder(\n",
    "                data=d,\n",
    "                label=d_id\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4224a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch = GridSearch(\n",
    "#     KRRWrapper,\n",
    "#     param_grid={\n",
    "#         \"bandwidth\": np.linspace(0.1, 4, 40),\n",
    "#         \"reg\": 1/10**np.linspace(1, 12, 20)\n",
    "#     },\n",
    "#     scoring=mean_squared_error\n",
    "# )\n",
    "\n",
    "paramsearch = OptunaHyperparameterTuning(\n",
    "    KRRWrapper,\n",
    "    param_grid={\n",
    "        \"bandwidth\": dict(type=\"float\", min=0, max=10),\n",
    "        \"reg\": dict(type=\"float\", min=1e-12, max=1e-3)\n",
    "    },\n",
    "    scoring=mean_squared_error\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbbce16",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict(\n",
    "    rand_score = [],\n",
    "    adj_rand_score = [],\n",
    "    silhouette_score_distance = [],\n",
    "    silhouette_score_clustering = []\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e7dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop\n",
    "for i in tqdm.tqdm(range(1000)):\n",
    "    datasets = []\n",
    "\n",
    "    for d_id, fd in enumerate(dynamics):\n",
    "        for i in range(n_sets):\n",
    "            # x0 = np.random.random()\n",
    "            d = generate_time_series(x0, fd, 1000)\n",
    "            datasets.append(\n",
    "                DataHolder(\n",
    "                    data=d,#(d - d.min())/(d.max()- d.min()),\n",
    "                    label=d_id\n",
    "                )\n",
    "            )\n",
    "\n",
    "    for d in datasets:\n",
    "        paramsearch.fit(d.data[:-1], d.data[1:])\n",
    "        bandwidth = paramsearch.best_params[\"bandwidth\"]\n",
    "        reg = paramsearch.best_params[\"reg\"]\n",
    "        model = KernelRidgeRegression(\n",
    "            kernels=[GaussianKernel(bandwidth=bandwidth)],\n",
    "            reg = reg\n",
    "        )\n",
    "\n",
    "        model.fit(d.data)\n",
    "        \n",
    "        d.update(\n",
    "            bandwidth=bandwidth,\n",
    "            model=model,\n",
    "            kernel=model.kernels[0],\n",
    "            alpha=model.alpha\n",
    "        )\n",
    "\n",
    "    # Clustering\n",
    "    similarity_matrix = np.zeros((len(datasets), len(datasets)))\n",
    "    distance_matrix = np.zeros((len(datasets), len(datasets)))\n",
    "\n",
    "    kernel = GaussianKernel(bandwidth=1)\n",
    "\n",
    "    for i in range(len(datasets)):\n",
    "        dii = (datasets[i].alpha.T@kernel(datasets[i].data[:-1]/datasets[i].bandwidth, datasets[i].data[:-1]/datasets[i].bandwidth)@datasets[i].alpha)[0][0]\n",
    "        for j in range(i, len(datasets)):\n",
    "            djj = (datasets[j].alpha.T@kernel(datasets[j].data[:-1]/datasets[j].bandwidth, datasets[j].data[:-1]/datasets[j].bandwidth)@datasets[j].alpha)[0][0]\n",
    "\n",
    "            dij = (datasets[i].alpha.T@kernel(datasets[i].data[:-1]/datasets[i].bandwidth, datasets[j].data[:-1]/datasets[j].bandwidth)@datasets[j].alpha)[0][0]\n",
    "\n",
    "            similarity_matrix[i][j] = dij\n",
    "            distance_matrix[i][j] = dii + djj - 2*dij\n",
    "            # similarity_matrix[i][j] = dij/np.sqrt(dii*djj) # Cosine distance\n",
    "            similarity_matrix[j][i] = similarity_matrix[i][j]\n",
    "            distance_matrix[j][i] = distance_matrix[i][j]\n",
    "\n",
    "    # Graph laplacian\n",
    "    D = np.diag(similarity_matrix.sum(axis=0))\n",
    "\n",
    "    L = D - similarity_matrix\n",
    "\n",
    "    evals, evecs = np.linalg.eig(L)\n",
    "    eval_idx = np.argsort(evals)\n",
    "\n",
    "    evals = evals[eval_idx]\n",
    "    evecs = evecs[:, eval_idx]\n",
    "\n",
    "    kmeans = KMeans(n_clusters=3)\n",
    "    labels = kmeans.fit_predict(evecs[:, :4])\n",
    "\n",
    "    results[\"rand_score\"].append(rand_score(\n",
    "        labels_true=[d.label for d in datasets],\n",
    "        labels_pred=labels  \n",
    "    ))\n",
    "\n",
    "    results[\"adj_rand_score\"].append(adjusted_rand_score(\n",
    "        labels_true=[d.label for d in datasets],\n",
    "        labels_pred=labels  \n",
    "    ))\n",
    "\n",
    "    results[\"silhouette_score_distance\"].append(silhouette_score(\n",
    "        distance_matrix,\n",
    "        labels=[d.label for d in datasets],\n",
    "        metric=\"precomputed\"  \n",
    "    ))\n",
    "\n",
    "    results[\"silhouette_score_clustering\"].append(silhouette_score(\n",
    "        distance_matrix,\n",
    "        labels=labels,\n",
    "        metric=\"precomputed\"  \n",
    "    ))\n",
    "\n",
    "    pd.DataFrame(results).to_csv(\"results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
