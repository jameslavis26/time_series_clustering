{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "1da326fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import optuna\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f9ddd30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2c1e1d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "d932c482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time_series import time_series_models\n",
    "from time_series import kernels\n",
    "from time_series import evaluators\n",
    "\n",
    "model_library = {name:model for name, model in time_series_models.__dict__.items() if \"_\" not in name}\n",
    "kernel_library = {name:kernel for name, kernel in kernels.__dict__.items() if \"_\" not in name}\n",
    "evaluator_library = {name:evaluator for name, evaluator in evaluators.__dict__.items() if \"_\" not in name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "80133e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "42f4b4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesData:\n",
    "    def __init__(self, X, y=None, train_val_test_split=None, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        self.N = len(X)\n",
    "        self.indices = np.arange(self.N)\n",
    "        self.tvt_split = train_val_test_split\n",
    "\n",
    "    def train_data(self):\n",
    "        train_idx = self.indices[:int(self.tvt_split[0]*self.N)]\n",
    "\n",
    "        if type(self.y) == type(None):\n",
    "            return self.X[train_idx], None\n",
    "\n",
    "        return self.X[train_idx], self.y[train_idx]\n",
    "\n",
    "    def val_data(self, lag=0):\n",
    "        val_idx = self.indices[int(self.tvt_split[1]*self.N) - lag:int(self.tvt_split[2]*self.N)]\n",
    "\n",
    "        if type(self.y) == type(None):\n",
    "            return self.X[val_idx], None\n",
    "\n",
    "        return self.X[val_idx], self.y[val_idx]\n",
    "\n",
    "    def test_data(self, lag=0):\n",
    "        test_idx = self.indices[int(self.tvt_split[2]*self.N)-lag:]\n",
    "\n",
    "        if type(self.y) == type(None):\n",
    "            return self.X[test_idx], None\n",
    "\n",
    "        return self.X[test_idx], self.y[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "1f43c9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelContainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel_name,\n",
    "        kernel_class,\n",
    "        kernel_parameters, \n",
    "        hyperparameters = None\n",
    "    ):\n",
    "        self.kernel_name = kernel_name\n",
    "        self.kernel_class = kernel_class\n",
    "        self.kernel_parameters = kernel_parameters\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "    def update_parameters(self, **update_params):\n",
    "        self.kernel_parameters.update(update_params)\n",
    "    \n",
    "    def build_kernel(self):\n",
    "        return self.kernel_class(**self.kernel_parameters)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "20032ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelContainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        model_class,\n",
    "        model_parameters,\n",
    "        model_kernels,\n",
    "        hyperparameters = None\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.model = model_class\n",
    "        self.parameters = model_parameters\n",
    "        self.model_kernels = model_kernels\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "    def update_parameters(self, dct):\n",
    "        self.parameters.update(dct)\n",
    "\n",
    "    def __repr__(self):\n",
    "        rep = f\"{str(self.model)}: {str(self.parameters)}\"\n",
    "\n",
    "        return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5779c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubExperiment:\n",
    "    def __init__(\n",
    "        self, \n",
    "        experiment_name,\n",
    "        model_container,\n",
    "        dataset,\n",
    "        evaluators,\n",
    "        kernels,\n",
    "        hyperparameter_evaluator=None\n",
    "    ):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.model_container = model_container\n",
    "        self.dataset = dataset\n",
    "        self.evaluators = evaluators\n",
    "        self.kernels = kernels\n",
    "        self.hyperparameter_evaluator = hyperparameter_evaluator\n",
    "\n",
    "        name = f\"{self.dataset.dataset_name} - {self.model_container.model_name}\"\n",
    "\n",
    "        self.results = dict(exp_name=name)        \n",
    "\n",
    "    def build_model(self):\n",
    "        model_class = self.model_container.model\n",
    "        model_parameters = self.model_container.parameters\n",
    "        model_kernels = self.model_container.model_kernels\n",
    "\n",
    "        # build_model\n",
    "        model = model_class(\n",
    "            kernels=[k.build_kernel() for k in model_kernels], \n",
    "            **model_parameters\n",
    "        )\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def tune_parameters(self):\n",
    "        model_mapping = {self.model_container.model_name:self.model_container}\n",
    "        kernel_mapping = {k_name:kernel for k_name, kernel in self.kernels.items()}\n",
    "\n",
    "        object_mapping = dict()\n",
    "        object_mapping.update(model_mapping)\n",
    "        object_mapping.update(kernel_mapping)\n",
    "\n",
    "        model_hparams = self.model_container.hyperparameters if self.model_container.hyperparameters else []\n",
    "        X_train, y_train = self.dataset.train_data()\n",
    "        X_val, y_val = self.dataset.val_data()\n",
    "\n",
    "        evaluator = self.hyperparameter_evaluator()\n",
    "\n",
    "        def objective(trial):\n",
    "            # Update model parameters\n",
    "            if self.model_container.hyperparameters:\n",
    "                model_parameters = {}\n",
    "                for hparam, hparam_conf in model_hparams.items():\n",
    "                    if hparam_conf[\"type\"] == \"float\":\n",
    "                        model_parameters[hparam] = trial.suggest_float(\n",
    "                            self.model_container.model_name + \" \" + hparam, \n",
    "                            hparam_conf[\"min\"], \n",
    "                            hparam_conf[\"max\"]\n",
    "                        )\n",
    "                    elif hparam_conf[\"type\"] == \"int\":\n",
    "                        model_parameters[hparam] = trial.suggest_int(\n",
    "                            self.model_container.model_name + \" \" + hparam, \n",
    "                            hparam_conf[\"min\"], \n",
    "                            hparam_conf[\"max\"]\n",
    "                        )\n",
    "                    else:\n",
    "                        raise ValueError(\"Expecting float or int\")\n",
    "\n",
    "                self.model_container.update_parameters(model_parameters)\n",
    "\n",
    "            # Update kernel parameters\n",
    "            for k_name, kernel in self.kernels.items():\n",
    "                if kernel.hyperparameters:\n",
    "                    params = {}\n",
    "                    for hparam, hparam_conf in kernel.hyperparameters.items():\n",
    "                        if hparam_conf[\"type\"] == \"float\":\n",
    "                            params[hparam] = trial.suggest_float(\n",
    "                                k_name + \" \" + hparam, \n",
    "                                hparam_conf[\"min\"], \n",
    "                                hparam_conf[\"max\"]\n",
    "                            )\n",
    "                        elif hparam_conf[\"type\"] == \"int\":\n",
    "                            params[hparam] = trial.suggest_int(\n",
    "                                k_name + \" \" + hparam, \n",
    "                                hparam_conf[\"min\"], \n",
    "                                hparam_conf[\"max\"]\n",
    "                            )\n",
    "                        else:\n",
    "                            raise ValueError(\"Expecting float or int\")\n",
    "                        \n",
    "                    kernel.update_parameters(params)\n",
    "\n",
    "            model = self.build_model()\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = model.predict(X_val)\n",
    "            return evaluator(y_val, y_pred)\n",
    "\n",
    "        study = optuna.create_study()\n",
    "        study.optimize(\n",
    "            objective, \n",
    "            n_trials=30, \n",
    "            timeout=60*20, # Optimise for n_trails or timeout seconds\n",
    "        )\n",
    "\n",
    "        best_params = study.best_params\n",
    "        # Need to map best_params back to model and objects\n",
    "        ## Also, different kernels with the same hparram name (ie bandwidth) will raise an error. Need to differentiate.\n",
    "\n",
    "        for param, value in best_params.items():\n",
    "            obj_name, param_name = param.split()\n",
    "            object_mapping[obj_name].update_parameters({param_name:value})\n",
    "                            \n",
    "    def run_experiment(self):\n",
    "        X_train, y_train = self.dataset.train_data()\n",
    "        X_test, y_test = self.dataset.test_data()\n",
    "\n",
    "        # Tune hyperparameters\n",
    "        tune_hparams = False\n",
    "        if self.model_container.hyperparameters:\n",
    "            tune_hparams = True\n",
    "        for kernel in self.model_container.model_kernels:\n",
    "            if kernel.hyperparameters:\n",
    "                tune_hparams = True\n",
    "        \n",
    "        if tune_hparams:\n",
    "            self.tune_parameters()\n",
    "\n",
    "        model = self.build_model()\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        for evaluator_name, evaluator in self.evaluators.items():\n",
    "            self.results[evaluator_name] = evaluator(y_test, y_pred)\n",
    "\n",
    "    def get_results(self):\n",
    "        return self.results\n",
    "\n",
    "    def __repr__(self):\n",
    "        rep = f\"Experiment:\\n {str(self.model_container)} \\n {str(self.dataset)}\"\n",
    "\n",
    "        return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "28ffe150",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "\n",
    "        self.sub_experiments = self.parse_config(filepath)\n",
    "\n",
    "        self.completed_experiments = []\n",
    "\n",
    "    def load_dataset(self, filepath):\n",
    "        data = pd.read_csv(filepath, index_col=0).values\n",
    "        return data\n",
    "    \n",
    "    def parse_config(self, config_path):\n",
    "        with open(config_path, \"r\") as file:\n",
    "            config = yaml.safe_load(file)\n",
    "\n",
    "        for experiment, experiment_confs in config.items():\n",
    "            models = experiment_confs[\"models\"]\n",
    "            datasets = experiment_confs[\"datasets\"]\n",
    "            metrics = experiment_confs[\"metrics\"]\n",
    "\n",
    "            # Load evaluators\n",
    "            hparam_evaluator = evaluator_library[metrics[\"hyperparameter_tuning\"]]\n",
    "            evaluators = {i:evaluator_library[i]() for i in metrics[\"evaluation\"]}\n",
    "\n",
    "            # Process kernels\n",
    "            kernels = {\n",
    "                k_name:KernelContainer(\n",
    "                    kernel_name=k_name,\n",
    "                    kernel_class=kernel_library[k_conf[\"kernel\"]],\n",
    "                    kernel_parameters=k_conf[\"parameters\"]\n",
    "                )\n",
    "                for k_name, k_conf in experiment_confs[\"kernels\"].items()\n",
    "            }\n",
    "\n",
    "            product = itertools.product(datasets, models)\n",
    "            \n",
    "            for dataset_name, model_name in product:\n",
    "                # Load dataset\n",
    "                data = self.load_dataset(datasets[dataset_name][\"filepath\"])\n",
    "                X, y = data[:-1], data[1:]\n",
    "                tsp = datasets[dataset_name][\"train_test_split\"]\n",
    "\n",
    "\n",
    "                # Load model\n",
    "                model_class = model_library[models[model_name][\"model\"]]\n",
    "                model_params = models[model_name][\"parameters\"]\n",
    "                kernel_names = models[model_name][\"kernels\"]\n",
    "                model_hparams = models[model_name][\"hyperparameters\"] if \"hyperparameters\" in models[model_name] else None\n",
    "\n",
    "                yield SubExperiment(\n",
    "                    experiment_name=experiment,\n",
    "                    dataset = TimeSeriesData(\n",
    "                        X, \n",
    "                        y, \n",
    "                        train_val_test_split=tsp,\n",
    "                        dataset_name = dataset_name\n",
    "                    ),\n",
    "                    model_container= ModelContainer(\n",
    "                        model_name=model_name,\n",
    "                        model_class=model_class,\n",
    "                        model_parameters=model_params,\n",
    "                        model_kernels = [kernels[k] for k in kernel_names],\n",
    "                        hyperparameters = model_hparams\n",
    "                    ),\n",
    "                    evaluators=evaluators,\n",
    "                    hyperparameter_evaluator = hparam_evaluator,\n",
    "                    kernels = {k:v for k, v in kernels.items() if k in kernel_names}\n",
    "                )\n",
    "    \n",
    "    def run_experiments(self):\n",
    "        for sub_exp in tqdm(self.sub_experiments):\n",
    "            self.completed_experiments.append(sub_exp)\n",
    "\n",
    "            sub_exp.run_experiment()\n",
    "\n",
    "            # self.completed_experiments.append(sub_exp)\n",
    "\n",
    "    def get_results(self):\n",
    "        return pd.DataFrame(map(lambda x: x.get_results(), self.completed_experiments)).set_index(\"exp_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "086931cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(\"experiment.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "7626d35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:04,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "experiment.run_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "07d446d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MeanSquaredError</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exp_name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dataset1 - model_krr1</th>\n",
       "      <td>0.238770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset1 - model_krr2</th>\n",
       "      <td>0.000507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset2 - model_krr1</th>\n",
       "      <td>0.055354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset2 - model_krr2</th>\n",
       "      <td>0.000507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       MeanSquaredError\n",
       "exp_name                               \n",
       "dataset1 - model_krr1          0.238770\n",
       "dataset1 - model_krr2          0.000507\n",
       "dataset2 - model_krr1          0.055354\n",
       "dataset2 - model_krr2          0.000507"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.get_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
